{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let\\'s go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"\\'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you\\'re gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"\\'re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I\\'m kidding.  You know how sometimes you just become this \\\\\"persona\\\\\"?  And you don\\'t know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"\\'m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\\\"\", \"tag\": \"\\'\\'\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n\\'t\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
      "b'{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\\n'\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"movie-corpus\"\n",
    "corpus = os.path.join(\"data\", corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "printLines(os.path.join(corpus, \"utterances.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits each line of the file to create lines and conversations\n",
    "def loadLinesAndConversations(fileName):\n",
    "    lines = {}\n",
    "    conversations = {}\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            lineJson = json.loads(line)\n",
    "            # Extract fields for line object\n",
    "            lineObj = {}\n",
    "            lineObj[\"lineID\"] = lineJson[\"id\"]\n",
    "            lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
    "            lineObj[\"text\"] = lineJson[\"text\"]\n",
    "            lines[lineObj['lineID']] = lineObj\n",
    "\n",
    "            # Extract fields for conversation object\n",
    "            if lineJson[\"conversation_id\"] not in conversations:\n",
    "                convObj = {}\n",
    "                convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
    "                convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
    "                convObj[\"lines\"] = [lineObj]\n",
    "            else:\n",
    "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
    "                convObj[\"lines\"].insert(0, lineObj)\n",
    "            conversations[convObj[\"conversationID\"]] = convObj\n",
    "\n",
    "    return lines, conversations\n",
    "\n",
    "\n",
    "# Extracts pairs of sentences from conversations\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations.values():\n",
    "        # Iterate over all the lines of the conversation\n",
    "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n",
    "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "            # Filter wrong samples (if one of the lists is empty)\n",
    "            if inputLine and targetLine:\n",
    "                qa_pairs.append([inputLine, targetLine])\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus into lines and conversations...\n",
      "\n",
      "Writing newly formatted file...\n",
      "\n",
      "Sample lines from file:\n",
      "b'They do to!\\tThey do not!\\n'\n",
      "b'She okay?\\tI hope so.\\n'\n",
      "b\"Wow\\tLet's go.\\n\"\n",
      "b'\"I\\'m kidding.  You know how sometimes you just become this \"\"persona\"\"?  And you don\\'t know how to quit?\"\\tNo\\n'\n",
      "b\"No\\tOkay -- you're gonna need to learn how to lie.\\n\"\n",
      "b\"I figured you'd get to the good stuff eventually.\\tWhat good stuff?\\n\"\n",
      "b'What good stuff?\\t\"The \"\"real you\"\".\"\\n'\n",
      "b'\"The \"\"real you\"\".\"\\tLike my fear of wearing pastels?\\n'\n",
      "b'do you listen to this crap?\\tWhat crap?\\n'\n",
      "b\"What crap?\\tMe.  This endless ...blonde babble. I'm like, boring myself.\\n\"\n"
     ]
    }
   ],
   "source": [
    "# Define path to new file\n",
    "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "# Initialize lines dict and conversations dict\n",
    "lines = {}\n",
    "conversations = {}\n",
    "# Load lines and conversations\n",
    "print(\"\\nProcessing corpus into lines and conversations...\")\n",
    "lines, conversations = loadLinesAndConversations(os.path.join(corpus, \"utterances.jsonl\"))\n",
    "\n",
    "# Write new csv file\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extractSentencePairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "\n",
    "# Print a sample of lines\n",
    "print(\"\\nSample lines from file:\")\n",
    "printLines(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 221282 sentence pairs\n",
      "Trimmed to 64313 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 18082\n",
      "\n",
      "pairs:\n",
      "['they do to !', 'they do not !']\n",
      "['she okay ?', 'i hope so .']\n",
      "['wow', 'let s go .']\n",
      "['what good stuff ?', 'the real you .']\n",
      "['the real you .', 'like my fear of wearing pastels ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['have fun tonight ?', 'tons']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "# Returns True if both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using the ``filterPair`` condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7833 / 18079 = 0.4333\n",
      "Trimmed from 64313 pairs to 53131, 0.8261 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[  11,   24,   44,  504,  111],\n",
      "        [ 265,  355,   24,   79,   24],\n",
      "        [2132, 1093,  443, 3970, 3257],\n",
      "        [3261,   31,  254,   10,   10],\n",
      "        [ 784,  126, 5494,    2,    2],\n",
      "        [ 254,   10,   10,    0,    0],\n",
      "        [ 805,    2,    2,    0,    0],\n",
      "        [  14,    0,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths: tensor([9, 7, 7, 5, 5])\n",
      "target_variable: tensor([[  36,   19,   34,   33,  280],\n",
      "        [  17,   42,   40,   66, 5932],\n",
      "        [ 358, 1653,   11,   17,   14],\n",
      "        [ 112,   10,   83, 4414,   14],\n",
      "        [   6,  146,   11, 2568,   14],\n",
      "        [   2,    6,  200,    5,    2],\n",
      "        [   0, 1293,  133,   18,    0],\n",
      "        [   0,    6,    5,   14,    0],\n",
      "        [   0,    2,   14,    2,    0],\n",
      "        [   0,    0,    2,    0,    0]])\n",
      "mask: tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [False,  True,  True,  True, False],\n",
      "        [False,  True,  True,  True, False],\n",
      "        [False,  True,  True,  True, False],\n",
      "        [False, False,  True, False, False]])\n",
      "max_target_len: 10\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size parameters are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # Lengths for RNN packing should always be on the CPU\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename, training_losses, validation_losses, validation_pairs=None):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "            \n",
    "        # Save metrics to file\n",
    "        with open(os.path.join(save_dir, 'metrics.json'), 'w') as f:\n",
    "            json.dump({'training_losses': training_losses, 'validation_losses': validation_losses}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "#``attn_model = 'concat'``\n",
    "hidden_size = 800\n",
    "encoder_n_layers = 3\n",
    "decoder_n_layers = 3\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Load model if a ``loadFilename`` is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For batched 3-D input, hx should also be 3-D but got 1-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Run training iterations\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Training!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainIters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m           \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_n_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_n_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m           \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloadFilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_accuracies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_accuracies\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[125], line 51\u001b[0m, in \u001b[0;36mtrainIters\u001b[0;34m(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename, training_losses, validation_losses, training_accuracies, validation_accuracies, validation_pairs)\u001b[0m\n\u001b[1;32m     48\u001b[0m training_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Calculate training accuracy\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m decoder_output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_variable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# decoder_output is the predicted probabilities\u001b[39;00m\n\u001b[1;32m     52\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m calculate_accuracy(decoder_output, target_variable, mask)\n\u001b[1;32m     53\u001b[0m training_accuracies\u001b[38;5;241m.\u001b[39mappend(train_accuracy)\n",
      "File \u001b[0;32m~/Documents/Personal/Univeristy/Bock 1/Assignment block 2/NLP Chatbot Project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Personal/Univeristy/Bock 1/Assignment block 2/NLP Chatbot Project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[112], line 27\u001b[0m, in \u001b[0;36mLuongAttnDecoderRNN.forward\u001b[0;34m(self, input_step, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dropout(embedded)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Forward through unidirectional GRU\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m rnn_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate attention weights from the current GRU output\u001b[39;00m\n\u001b[1;32m     29\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(rnn_output, encoder_outputs)\n",
      "File \u001b[0;32m~/Documents/Personal/Univeristy/Bock 1/Assignment block 2/NLP Chatbot Project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Personal/Univeristy/Bock 1/Assignment block 2/NLP Chatbot Project/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Personal/Univeristy/Bock 1/Assignment block 2/NLP Chatbot Project/.venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1083\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 1083\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched 3-D input, hx should also be 3-D but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1085\u001b[0m max_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1086\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For batched 3-D input, hx should also be 3-D but got 1-D tensor"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.8\n",
    "learning_rate = 0.0004\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 500\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have CUDA, configure CUDA to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename, training_losses, validation_losses, training_accuracies, validation_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[8.968024972044391, 8.36903218878607, 8.318228882915403, 8.335677177517343, 6.423717540617172, 5.7762250933869215, 5.594347044725277, 5.362391512601403, 5.284091147970646, 5.743408599575265, 5.594760967423106, 5.532053623175531, 5.548572032528923, 5.365945295177603, 5.386338897307001, 5.329274646673106, 5.3438830613495405, 5.250838337610838, 5.244250387104252, 5.003201724693274, 5.212834534198237, 5.145472814216938, 4.830759461479211, 4.8078279493683, 4.899642680303066, 4.874684287852879, 5.226906948963904, 5.149032121612912, 4.873836302070091, 5.150257973565733, 4.890431424957983, 4.906981458076008, 4.929486095012351, 4.970345377702655, 5.431323313769975, 5.040488888296599, 4.933161656832665, 5.905355279849735, 4.900811425362205, 4.736910526931976, 5.009293941012893, 4.741126669468867, 4.616740068045255, 4.597178756391714, 4.672639975886719, 4.901421440664188, 4.857410258900861, 4.607827897073441, 4.620023075823054, 4.574004824196827, 4.775053171480491, 4.911127219048257, 4.858844479697332, 5.244590066640805, 4.702892626645534, 4.861384611290235, 5.315386269077565, 4.981264458695039, 4.876179485226304, 4.727127591755031, 4.923550645009266, 5.169655665247919, 4.717703066017262, 4.84009668822961, 4.868369052979427, 5.0035865816840515, 4.684002229663992, 5.311265475776892, 4.804197288942887, 4.7802084076032045, 4.688968889942654, 4.669157782013033, 4.6368598824912555, 4.557734874430402, 4.634777817116733, 4.694425671720962, 4.652474722179868, 4.483797656102596, 5.141013820245476, 4.749906699287097, 5.120135343707086, 4.655539010621217, 4.553348526008903, 4.686502941869864, 4.330110097354878, 4.6290378655624576, 4.492251261055319, 4.64159228888618, 4.621030764634672, 4.663027776119191, 4.649807659291287, 4.515598519646352, 4.570361323371718, 4.349174182166529, 4.353873087959864, 4.518609443078825, 4.594033621710413, 4.396284126243362, 4.654558079512528, 4.538765328406027, 4.671993194900167, 4.8714949523609965, 4.567647285283979, 4.465606997458999, 4.550904972843789, 4.400891125553714, 4.320611890509158, 5.043550519529686, 4.584689174718525, 4.7240193927729575, 4.450840891452579, 4.453098730469989, 4.496356629733627, 4.355288412565044, 4.57571166572462, 4.495269595168933, 4.615050528068095, 4.421411880481645, 4.423652240601765, 4.38602140310858, 4.59635598784799, 4.349769961548692, 4.462419803208876, 4.410761628203305, 4.492226949292276, 5.460411127875833, 5.02617197979539, 5.361460703659648, 5.023998840222441, 4.692283384812375, 4.730649436174369, 4.516267837190688, 4.810966618535643, 4.366951471187282, 4.540918175284559, 4.6058756484182, 4.518078064349794, 4.356567029651226, 4.505204860238545, 4.67173110037354, 4.278752584534524, 4.390613756078608, 4.517062142178213, 4.371353006761983, 4.256577139428918, 4.396032731473371, 4.40712127964295, 4.317419162446752, 4.1383276543964875, 4.392672415910999, 4.36800309587767, 4.3263770496616, 5.627848300025287, 4.3428517058397755, 5.657244993775499, 4.788463700699988, 4.397455165988391, 4.4338202264622595, 4.5144009490092945, 5.217350332166559, 4.4421903262023, 5.009775393949649, 4.971599477775318, 4.695231735630303, 4.653080231830141, 4.5014013921764935, 4.383980798610839, 4.476603899411258, 4.395298995482533, 4.402694551780413, 4.409481132478436, 4.541382410691219, 4.116142691765221, 4.3992413964573664, 4.4515337711421585, 4.2027081136091855, 4.200270965044173, 4.098409647117286, 5.759237913990983, 4.441795264600365, 4.392595474420176, 4.146261872439729, 4.458981843544058, 4.2448865367246755, 4.432973576829501, 4.4302560777450735, 4.156089569537402, 4.264756834953462, 4.435190471228521, 5.72480610923533, 5.287606388929501, 5.089075356059604, 4.515124679465846, 4.399218821971435, 5.031770782176511, 4.413211687615044, 4.514364082108504, 5.186539497161447, 4.510752693267923, 4.968716417105143, 4.413194593734777, 4.9290081686892755, 4.962480160810798, 4.411279305124791, 4.344016151128316, 4.299404962126564, 4.994026268509176, 5.182867727524195, 4.31384565685989, 4.136485491048905, 4.2450371960930475, 4.204401859919802, 5.182423257888657, 4.065137134766893, 4.200527431675882, 4.398911198623732, 4.0390838375480245, 4.179881735936796, 4.277708102957062, 4.241037211766356, 4.1195649646023424, 3.9813914067799256, 4.080194617270752, 5.425371074461722, 4.189456825560712, 5.2279295004446205, 4.357020335637179, 4.122727163705536, 4.2272465585833405, 4.178283482023763, 4.383313288470661, 4.063344180623271, 4.145024377489404, 4.152225419155394, 4.205576599916247, 4.309909425600611, 4.018296047813534, 5.2328088325683515, 4.136270605846367, 4.22474355896314, 4.28556142074507, 4.1134297112098, 4.2536823083997595, 4.03889917250185, 3.9584904784144777, 4.0235106138587, 5.34138348611519, 3.991366350949193, 5.485010026051448, 4.466675366814426, 4.18792296128567, 3.925128511393503, 4.270886556781018, 3.933326165763267, 4.25357907753284, 3.8948882823514808, 3.9749298864927693, 3.930653580642156, 4.045316105548617, 3.985280546387215, 5.575317516735279, 4.098024032392316, 5.065776159365972, 3.969581240824714, 4.266672557691028, 4.0741551624877115, 5.251308631415319, 5.293149205463566, 4.2323475785638225, 4.218379848193927, 4.235049569872679, 4.224351500439552, 4.1945402562067065, 5.054292824057161, 4.015883432977953, 4.102086996760892, 4.143822218779071, 4.824228070334726, 3.9617009383454844, 5.1333453095522685, 4.029240304870265, 4.100670468181204, 4.085864909556694, 4.137728867340397, 5.659519617294564, 3.805970426122555, 5.061832085517223, 4.00932807110441, 5.195694688636224, 4.0385851625036056, 5.059041219515417, 4.1269366674695345, 4.10688619021594, 4.038074024524794, 4.111409573435291, 4.126057403671498, 4.270111660521055, 4.053989076791204, 4.188535677280584, 3.9191793009811633, 4.080311739667876, 3.9458509498536967, 4.052529999907588, 4.010767022013211, 3.8893009672166663, 4.123222945878903, 3.8479710706362598, 3.8508081768046725, 3.9475662193030043, 3.925703534703842, 3.892939450350836, 4.110478318801948, 3.8461711991559877, 4.094738615429507, 4.051645887422968, 4.194658561309581, 3.9102871773530774, 3.7356444937780746, 4.044358252578161, 4.011439599144352, 3.8868040707837292, 3.9660653792542475, 3.783777986093309, 3.8245933596892536, 3.8210918711757738, 4.0886795158015214, 3.7748510854799044, 3.781527223896068, 5.359424754332553, 3.796857390694226, 3.8809941552407907, 4.0881475245611405, 3.9243995220840313, 4.058765344801463, 5.12338717206178, 3.9017741063004965, 4.0534055996474825, 5.144632781532935, 3.97893592922115, 4.174816310039233, 5.127867771092282, 3.7715135331902214, 4.089792961788963, 4.108127767169472, 4.021663918237018, 3.9452935458765817, 3.9806365708334255, 3.9792444621650307, 4.041591146486637, 3.7576193919594805, 3.981127546708675, 3.918649285161328, 4.055505604583484, 3.820081087402609, 3.866431384135562, 3.8751103676394196, 3.983054741356278, 3.9770395237784486, 3.7938055266603463, 3.9369333825574735, 4.0879551072114255, 3.838952461209623, 4.0120169391440355, 3.8963990672102455, 3.9058917888800666, 3.6812142920331556, 3.9922333738544293, 5.67976859241724, 3.833510887832723, 3.8850153461435584, 5.204426958044963, 5.157387900560057, 3.849020318403042, 4.040697536210424, 4.073212727299197, 5.059267331118607, 4.101967911291868, 4.136002425935918, 3.9672231595351195, 3.8948201835813165, 5.0548595205879465, 3.9977561873941676, 3.931797513073566, 3.836114310051281, 4.092796669078819, 3.8779335785899067, 3.8476669874037657, 5.246249440118103, 4.121209606927421, 3.773395692136618, 5.093165483731062, 5.1340917867392, 3.760272922511092, 3.84346020969594, 3.932758141724004, 4.06215681278442, 4.910017153302443, 5.045963040960702, 3.9526235566503067, 3.9184150636907598, 3.8827109890527853, 4.030982229002633, 3.964364166157719, 3.8548705616418055, 3.9581303029230925, 3.769708344808892, 3.873839497667906, 3.6946700958987333, 4.053114554272649, 3.848978462436539, 3.7720444643437507, 3.997031377468244, 3.898635876902118, 3.683563967384828, 3.8822559957974123, 3.6946469034609914, 5.4345067692187525, 5.505215346058713, 3.9438833194993186, 3.674360750298346, 3.8297119784671305, 3.8919701378141376, 3.798200651700342, 4.941468594779431, 4.0592922718072355, 3.833874163721413, 5.069385380309245, 4.0208617380875, 4.775853898915612, 3.743944385636252, 3.97390798984526, 3.861470434521475, 4.9869224565246935, 5.227816194711378, 3.9065206835739374, 3.945500834962866, 3.9014187328328243, 5.025126683029963, 3.7869312232418673, 4.848520073675016, 3.852601154060925, 3.851414617285787, 4.03672930483475, 3.818752943419237, 3.981705188651356, 3.7753442805891004, 3.670170336457936, 3.78066246130112, 3.733951193956554, 3.6823721288979203, 5.209903744206284, 3.7037643624794097, 3.822640577480197, 3.9282255082627264, 3.670084426076593, 3.833297735299258, 3.8764493347955318, 3.6623921271904494, 3.9340325966755403, 3.9777770095912652, 3.847970153802549, 3.9063067523188386, 3.5134314521219334, 3.777834164836401, 3.6664638133384626, 3.4986926126083238, 5.414622471947681, 3.6192812583211698, 3.7546690733674963, 3.742732879707908, 3.733681511999214, 3.81751395843529, 3.6321988871615183, 3.675853119921181, 3.8355239525544045, 3.826074160823753, 3.71840094016392, 3.696984655695197, 3.7097095462804037, 3.7753804086963236, 3.90313597398762, 5.191899193419725, 3.6882024352564247, 3.67810822511268, 3.6355628101469635, 3.8042736643859194, 3.8490936427610283, 3.822713787668599, 3.7096428802954056, 3.5317684981061137, 3.8188942158836, 5.39603311870948, 3.815928361427874, 5.107363584220263, 3.7219239715210133, 3.42272955959396, 4.656709903992978, 5.008098737046874, 3.8891050889279213, 3.795851660379465, 3.9618225971740837, 3.658438532920356, 3.7779112122861407, 4.877743542311545, 3.829017641417012, 3.6201825877651572, 3.786266958549002, 3.6521596916084067, 3.8028820542149, 3.9166675137884823, 3.811902809098452, 3.8018843330470706, 3.7009282713797926, 3.590610778974552, 3.890271729610574, 5.370258874946449, 3.7636453182788556, 5.253411878927539, 5.35271726958942, 3.8424159274691148, 3.725044023235718, 5.040631747065623, 3.7312723777276475, 3.77794550246871, 3.7741032739607534, 3.7455029817011494, 3.7543627542903155, 4.9981052184506405, 3.8262790680024192, 3.806140566722622, 3.8069903506653304, 3.667255814860855, 3.8291013984004953, 3.675141304213414, 3.6663092384475298, 3.666177344668296, 3.7538696880141895, 4.633911236069015, 3.622214899241629, 5.079542689389072, 3.6807533430066246, 3.7629123581479296, 3.7263502856070767, 3.735509229800249, 3.5381321107501824, 3.6332184391780564, 3.7649339027761752, 3.7620038153596607, 3.706451271211511, 3.767395162015104, 3.7353308830813945, 3.6600261722293603, 5.0642077173778715, 3.759925826941137, 3.3362039400045087, 3.729683686500218, 3.3829035686278663, 3.5880383302479575, 4.938530846011071, 4.923893715710616, 3.5884485902206507, 3.6833447110153243, 3.725441253441474, 3.8180997449062324, 3.6329047992785073, 4.980285392442555, 3.6224536695362186, 3.448427487799324, 3.918774005822578, 5.020967759981328, 3.7255503619875747, 4.867277476246372, 3.8299321789461525, 5.0223269892377775, 3.5598499818541245, 4.02153047469036, 3.5891061139076066, 3.788580380400063, 5.182622674132595, 3.8620789503660418, 3.58842566362264, 5.008755853084417, 3.75773765693737, 4.927670472165195, 3.775121469367594, 3.849296368453138, 3.720419136756481, 5.0568105408453174, 3.9660896894358495, 3.62294751869293, 3.5797249562454083, 3.6636122602186663, 3.786592358597239, 4.98515832620083, 3.7494747819527983, 3.914031480890813, 3.8811879515722487, 3.699038618933592, 3.8780667009130982, 5.082892535123062, 3.6351020047676013, 3.438323732284079, 3.5120936732648116, 3.4810214596888813, 3.72864775558749, 3.655317122558025, 3.650590778411845, 3.485276398448114, 4.973915238716663, 3.311372747656768, 3.65840502202204, 3.7125355795948822, 3.5946352019870482, 3.6602265867983137, 3.454640898356185, 3.602597670353228, 3.7099390162220884, 5.298545660356656, 3.4972602241320376, 3.538321894061005, 3.4436312736404178, 3.575656287343428, 3.5029563072287595, 4.881726928363461, 3.6288893385307768, 5.017629851885187, 3.64589294557853, 3.6098794719840908, 3.685341083835977, 3.720577518209227, 3.649259318746911, 3.561372785898765, 3.820664209349147, 3.5467967307310615, 4.88551245995012, 3.585735272237036, 4.7811267307826455, 3.6714583144334085, 4.664107916972977, 3.661166399985131, 3.343060110712668, 3.665336209486506, 4.674201670262191, 3.5524117985067885, 4.612025721330093, 3.6418855856118664, 3.5854758996014513, 4.66495859022161, 4.736775151341261, 4.697035782539648, 3.5943908496653734, 3.6263396851334377, 3.684579296839962, 3.731021130269812, 3.5308549217109686, 4.842352069078147, 3.7996767107064704, 3.4826468803896478, 4.696634146273136, 4.847430437842023, 4.788041696512609, 3.6141475742687326, 3.580952390297674, 3.595742148502828, 3.6686058880872503, 3.6760478651434405, 3.6277019194636138, 3.5328273496457507, 3.312741025455413, 3.6035964449683995, 3.6645436848585424, 3.4757713003300306, 3.470704619581811, 3.711974486768565, 3.4428332666033445, 3.5515488889664777, 3.62798526722156, 3.385610845260884, 5.239505667305782, 5.2651946892340975, 3.527190081667599, 3.475878485060971, 4.756930209817113, 3.642333308218159, 3.711824574198928, 3.6338814370951815, 3.4137347398820936, 3.539984241677826, 4.9303049216270445, 3.6142141698293533, 3.577865357967554, 5.034667670961133, 3.500621245317398, 3.5388908793543155, 3.5865436623955427, 3.7227000852589898, 3.477081938194377, 3.4212831550036666, 4.8124837379553504, 3.4591077130322607, 3.711522072443119, 4.860788945672807, 3.4876987772747894, 3.5880170809002445, 3.7027231490838766, 3.684297602902054, 3.434073868881318, 3.5683004241786747, 3.589317832641285, 3.3963185132464817, 3.274447307146861, 3.5591274821765664, 3.5352942237456704, 3.5641984549908914, 3.558648325057712, 3.1729079608729376, 3.35573780780331, 3.378560964205815, 3.4967143251518675, 5.150322526311979, 3.474732749957543, 4.950037230382261, 3.601379495597085, 3.556666463319166, 3.563527430336441, 3.396224647015333, 3.616169507891646, 3.7138895529744387, 4.956863034936719, 3.355748431430753, 3.6684599178557624, 3.616622830084974, 3.548713662272986, 3.3746722838486813, 4.857000752420209, 3.5089139690813846, 3.630499227288241, 3.569649042255262, 3.304841450472727, 3.3093553711294983, 3.5560091865477546, 3.4762026980574348, 4.817739560314339, 4.819065775663134, 3.4089595773713848, 3.432393806156494, 4.888451751491094, 3.5372047216475657, 3.4213211580326064, 3.7833821330510116, 3.3402408714720746, 3.43659162591668, 3.4682920133823125, 3.46752843733397, 3.3684825080434804, 3.5681671879372305, 4.852047646425369, 3.383657876709304, 3.4285816519424834, 3.4381584304853594, 3.3791814279820533, 3.3002144545794296, 3.5607593856049036, 3.3663591896705185, 3.521169565982863, 3.6819169272381185, 4.950981322358705, 5.09871935406048, 3.4161769225122294, 3.4401610095483, 3.454486897958375, 3.5323101943529935, 3.5262736399485735, 3.2743958128143196, 3.5085371285486406, 3.416507001369813, 3.314665817075474, 3.4598522815313046, 3.5308462062024635, 3.554279193570537, 3.4655009489934967, 3.5193481049588264, 3.3980220198327955, 3.4618077777280396, 3.5250405418325443, 3.0443578239530327, 3.24792632805773, 3.2347013955065944, 3.5891454381586962, 3.4978591951618627, 3.188346664773315, 3.3517801155782725, 5.118282202276445, 3.413902214795079, 3.2275662121076607, 3.323028801699376, 4.995332982097267, 3.4733846909332393, 3.5557922274634546, 3.2877480383933135, 3.5753454639267526, 3.5918490056386765, 3.357924759690392, 5.089766064030371, 3.427372757122478, 3.443770207731774, 3.422480606312066, 3.273058315713198, 3.3422216956461757, 3.36469511205205, 3.3623306747949706, 3.4049329639508805, 3.3030686098917394, 3.489574661439324, 3.238319251120439, 3.466817485081408, 4.933976572137981, 3.4080873952373922, 3.3385105412188354, 3.1887912778978706, 4.873027875691579, 3.4875818538810592, 3.384346460853828, 3.442768835696335, 3.296306924971402, 3.516789691937675, 3.25476570942327, 3.4042930066615202, 4.791955757080831, 3.399447698735309, 3.2059757170514334, 4.686011755300454, 3.3901605834521313, 4.80478090968589, 4.466956121175725, 3.517683869365401, 3.5141200767841556, 3.5414986038321064, 3.442586479908544, 3.3991840676885414, 3.3545339926235074, 3.6773503130094873, 3.363325364744983, 4.5815033905911, 4.634161654271577, 3.575303508446807, 3.416197991431691, 3.6262358635720005, 3.540342024950917, 3.7851977316995122, 3.467703867897751, 3.4682934360317272, 3.2781224001508615, 3.6746485599150738, 3.286827454567051, 3.604734980383315, 3.2550602369404293, 3.367206568364054, 3.3209964994983783, 3.34242658440596, 5.151018502006543, 5.016285077133328, 3.543907830205101, 3.4392765924707054, 3.4301390970488645, 3.2285730480559818, 3.3886747827071, 4.745348822196018, 3.2897417808420117, 3.2810093716601143, 4.485181910981053, 3.403757469826561, 3.4323087955790714, 3.42674006759604, 4.67485448150231, 3.3883539021015165, 3.368363660119656, 3.3661703643318948, 3.367483804207791, 3.491769417430343, 3.319357708979675, 3.3873960641874645, 3.2811697042956474, 3.318619701836868, 3.356453790035557, 3.4027690349369855, 4.9511382062713825, 3.257928748787361, 3.47104008501313, 3.346549137949554, 4.836780147507063, 3.2985617571348507, 3.4059376355813398, 3.180962837872311, 3.280541773250207, 4.746003146510391, 3.536759832529677, 3.3261233759659095, 3.5457614925951337, 4.767442049983057, 3.3783484667654515, 3.463569684419781, 3.1880196710824125, 3.42798994453904, 4.8341116948963965, 3.2488422048215573, 3.357347815045418, 3.270877207958015, 3.617844747746416, 3.269791875321512, 3.392864363361031, 3.230756656402811, 3.30738821431581, 3.4549711717132685, 3.4002764093216937, 3.3315793148295016, 3.2712819339419084, 3.3897400350361275, 5.1381090893596415, 3.102902724126611, 3.057257769037356, 4.993012716635218, 3.3177488080135227, 4.732634786014086, 3.1363156011106867, 3.5277937247092677, 3.4814856971832033, 3.4702648325420418, 3.30989329268535, 3.399952156851068, 3.43246577255527, 3.2183021860024366, 4.745671009689382, 3.1558206527001693, 3.202123476136892, 3.272799715249225, 4.61016840771916, 3.224728112585966, 3.3939512398655727, 3.3119576657914744, 4.77036704460672, 3.245885210783303, 3.3532327135829147, 3.3449321551124878, 3.5017649607658385, 3.1815380837183507, 3.383973024034685, 3.3283078374144455, 3.0414376996540464, 3.4295724313820433, 3.267192759445998, 3.4280298067764803, 3.4920324861605962, 3.3065984684033833, 4.768293013560371, 3.299947748515741, 3.294383942019994, 3.201829180086617, 3.3354257128658187, 3.3020178946372, 3.285051689146561, 3.314157710006141, 4.737477312511315, 3.158844884495157, 3.363668804836546, 3.4274285644085083, 3.1337675098169995, 3.2699381323582535, 4.888303542607709, 3.3980375971882264, 3.2381038455213615, 3.2488997296064293, 3.0891659529602444, 3.2681494093905363, 3.4130521957360376, 3.327298275014003, 4.7376442400515835, 4.91168543989764, 3.478971236176286, 3.0551700588599173, 3.131602965336847, 3.445164457435759, 3.5145780353134355, 3.383878022913085, 3.3029083461492625, 3.286716334635375, 3.217590740752851, 3.2226631583398753, 3.2652770472156942, 3.3297206617668658, 3.257182488177212, 2.9925048235501164, 3.3194163336818363, 3.2599080864182137, 3.239193515808061, 3.3239141536481385, 3.229888146492391, 3.153489204779888, 3.2289406332346977, 3.177827161154412, 3.247078903297988, 3.1488966943549404, 3.2581003580388948, 3.274299682096698, 3.1787529590999832, 5.038988595665839, 3.098662111334717, 4.840784495144444, 4.779847989870808, 3.104147906469948, 4.943138986681447, 3.113678566285712, 4.88659868970994, 3.526025700258633, 3.5165977241457815, 3.218574816993124, 3.1586281508635516, 4.582810340185653, 4.5361472507441265, 4.592326661948326, 3.3761211422612756, 3.3370331187646585, 3.3552931304884424, 3.379251904461118, 3.456356723918833, 3.2539146658344755, 4.387505646340259, 3.2413353590045277, 3.378748459810121, 4.665700658391684, 4.633713634144855, 3.3512191307336954, 3.3218189096652613, 3.3348371711716727, 3.2818040132285686, 4.779018818256227, 3.305191247360469, 3.2056482175613756, 4.860339844273222, 4.804899720661798, 3.4396352398587813, 4.61276312582242, 3.396966143766087, 3.1509736007532307, 3.1483167639671215, 3.197591148367073, 4.556797174086985, 3.5178959243429038, 3.363705947629867, 3.2648253578962905, 3.4388575602033185, 4.671616540283482, 3.3592888486200296, 3.378278025686377, 3.2913514868201452, 3.210989498724247, 3.1771915512648667, 3.2026980883686456, 3.1239797552528676, 3.4393037578153596, 3.1276322089379622, 3.1988779336504347, 3.343756544403732, 3.1853582046635918, 3.3204531112284053, 2.9944934105248735, 3.3207849634404716, 3.4018947681684852, 3.3833678781933707, 5.153522252204454, 3.214121517052396, 3.3186214165908465, 3.2787509734828046, 3.1804913190879835, 3.2771187102982346, 3.29263649983926, 4.941556698061328, 3.2350805367334865, 3.18955775775085, 3.324859140121859, 4.742279644473599, 3.209280480570104, 4.6463711629983475, 3.2538324982891065, 3.198411445479097, 2.9939931882820745, 3.0432662860302213, 3.439006388422003, 3.3864117967663017, 3.207487318993695, 3.0718148192511125, 4.589269932053333, 4.603104011451497, 3.1186645839203355, 4.679040743328219, 4.734787052604044, 3.247447670255907, 3.2635006842654457, 3.4627376385175497, 3.101693332118571, 3.068614624749103, 3.251658760283541, 4.7153809642013975, 3.1578206900630326, 3.071267817203338, 4.692247209421063, 3.3737499987116513, 4.491018887147745, 3.174576882854309, 3.245480570692239, 3.1704784563962063, 3.168382090899043, 3.202974097122012, 3.0192926858207243, 3.2246082290830347, 3.384226289909043, 4.888539040043038, 3.007376070532835, 3.141252044208196, 3.224849370840942, 3.206372507513461, 2.983558117675704, 3.315759902141456, 3.2831919646818184, 3.408343581804968, 4.933793954356373, 3.1162519613812725, 3.3389040570202897, 3.223992251001119, 3.3109731674241374, 4.72216188854675, 3.3055625377409386, 3.073035583022497, 3.271423875593989, 3.0350909874911234, 3.203964726139545, 2.9612424100407195, 3.396735676354873, 3.071144649011885, 4.755169328779746, 4.874396776178069, 4.521732054800544, 3.0449381754138414, 3.162402729415775, 3.14239302009159, 3.199571309901362, 3.105485366384066, 4.559438480774767, 3.315454431246909, 3.16482427814476, 3.094829882535175, 4.68131618857687, 3.2278817513453486, 3.2418148286371347, 3.164514629163467, 3.036534244734003, 3.5156135705607046, 3.097647398906974, 4.625157336426372, 3.162879292553221, 2.99798095251219, 3.125645921057805, 3.1262484157324733, 3.1513881603827203, 3.1632619309675647, 3.179111954901911, 3.093933994420258, 2.848593744211235, 3.2043572226191746, 3.198849339996674, 3.0384370040826965, 3.161808397295186, 3.0790952653539008, 3.1118605558508596, 3.0511067619622505, 2.98803016312392, 2.9768579456550373, 3.3410931439875395, 2.9182543180372242, 2.99620062709985, 2.999510675366538, 2.952170996655862, 5.1290393476308695, 3.057891573138486, 2.8884267594936404, 3.186815097208439, 3.1483307661479003, 3.167579517430332, 3.0572450232978623, 2.956338350787681, 2.914175780989089, 4.833235159860208, 3.307641099671038, 4.860497335462194, 4.572499804992578, 3.1480089424032136, 4.809154277229552, 3.2960393923481113, 3.1641028896497887, 3.318484502442172, 3.1179233783929403, 3.113129240412934, 3.2168922031229457, 3.0483746655744826, 3.135493937501599, 4.865462366172245, 2.9540488028850485, 3.264633985653297, 3.224163297081801, 3.1452897820606767, 3.096542527794743, 3.0634074472394346, 3.2928048664956804, 3.0365497957294187, 3.418377943994096, 2.9032388922988477, 2.807958362015986, 4.736631497624632, 3.098861152781625, 3.0660607314606825, 3.0483405455485966, 3.0310697198110192, 3.1178389217227056, 3.1816507420282214, 3.0802289627658612, 3.174738288122754, 3.0614375546434367, 5.11227790566717, 2.9586622224990826, 3.1890283645148956, 4.73865254589405, 3.035995819445314, 4.6648956749655985, 3.146820050248434, 4.973465790824284, 4.498972030435167, 3.311903537655827, 2.975375624136068, 4.545295897992294, 3.312915474049104, 3.1684462915233396, 3.11194104811883, 3.239004864081602, 3.059116903225695, 2.9648364023476135, 3.1057494289277505, 3.178837302298128, 3.1822843745865987, 4.6965862715786155, 3.0243356454521026, 3.2501072898307295, 2.998594637507875, 2.8839697016450967, 3.0697006681438688, 3.191982062101098, 4.5991100986358715, 4.688708139068885, 4.855560051244926, 3.1792416259244716, 4.470207318048155, 4.661590342242996, 3.2236374853365124, 4.184931512504932, 4.286640387676753, 4.312281174963389, 4.3756230870363915, 3.4265461374687245, 3.242097573687542, 3.314224478051377, 3.321031367699401, 3.1171844684357977, 3.2860671435423554, 2.9635035923157615, 3.2404648280188217, 3.179898279453573, 3.1114378201782085, 3.2432229640739414, 3.257364263948802, 3.0083194902825463, 3.0900813582927786, 3.315505109872255, 3.1954761071112725, 3.2363110523172733, 3.2242911040609212, 2.983614886781344, 3.0164632342431856, 4.80290412605552, 3.105968914554488, 4.706028321502256, 3.1187969594636464, 3.1101900444684247, 3.0914792925819716, 4.5171936070297996, 3.065787952741966, 4.6380737078737235, 3.012637097187398, 4.67892461903672, 3.064649806935818, 2.9009600335355636, 3.2463703820919765, 2.972923071088293, 4.51143192127347, 2.879491082907343, 2.9930077716160692, 4.414166585173955, 2.897492646640812, 3.066682421804614, 3.2226446018050137, 3.002420038249283, 3.090739398430252, 2.991924214596048, 2.9878693698557566, 4.669844298456459, 3.191496783246597, 3.155987442455675, 2.992396910187848, 2.994402491348829, 2.921646959024919, 2.8922651481901047, 2.9906841534095467, 2.9662064929385266, 2.8531605167128147, 3.2237749848136446, 3.045545168830972, 3.0480786784337117, 2.878458828163302, 2.9059156399411648, 2.9265838044258783, 2.9667905344366052, 3.0263782906904817, 4.926459134196778, 2.9039127777372635, 4.868812193002815, 4.846620499035449, 3.2247248555793138, 3.059325858700982, 3.049665522280006, 3.064367434995324, 3.10657320036086, 2.973020153089778, 4.658747500287364, 3.3186161490995554, 3.1941261825111744, 4.667050018965569, 3.0910255261925492, 3.1821616174529956, 3.153740281035425, 4.299002419460444, 2.918536211952878, 3.069252391507616, 2.821932206033572, 4.621324964803667, 3.028354538234684, 2.9511271103247116, 4.376610795426706, 2.874935908056103, 3.0012934292340048, 3.0644014266945847, 3.203352046244716, 3.1080123471059, 2.896072157335707, 2.8575958905227403, 2.8554131169005834, 2.841578475337955, 3.1335837861913585, 3.0666625721925738, 3.093466804139776, 3.3361067804490685, 4.569619644973497, 2.9454408799409095, 3.050103261252081, 3.1133489787428994, 2.9375641581300016, 3.1289201001702893, 3.11054649257689, 2.808796193198187, 2.987119676943223, 3.019119879007704, 2.9809541771274635, 4.792982448408526, 2.8841161561118165, 4.829291285452417, 3.2162577791557627, 3.007934245162474, 3.0256964869584104, 4.587285584225834, 4.727620563920467, 3.072155427710213, 2.956223144582242, 3.019382635827873, 2.953555082523148, 2.9590529754102506, 3.067145191900132, 3.0907369517588523, 4.4747427901362675, 4.510751074854879, 3.084029110415136, 2.9151567096326, 3.0191218742040777, 3.017865845330682, 3.137205453335873, 4.5041198146885755, 3.0909227169783415, 3.0121264535682806, 3.024405650340817, 2.9509051396596377, 3.0596446802844284, 3.127144269671342, 4.459569965034051, 2.9896387361792827, 2.9222979281083266, 3.3098864867191278, 2.9951802665870564, 2.998916601642005, 2.917536010087463, 3.0320752514630307, 2.97908452215768, 3.012334425238085, 3.0381465518931825, 3.1488132811676808, 3.179865469879318, 2.9463389755425338, 2.9641516982777483, 3.0379608043405257, 2.979367786839318, 3.0658384020208875, 4.969526044298165, 2.9680917244711456, 2.8706281110267096, 2.949511207992242, 2.9092077514433092, 2.9565372028869326, 3.0326396097677835, 4.945206183840515, 4.697020855549863, 2.9219095670789224, 3.0344119175799986, 2.9223132659284707, 2.9111716209404843, 2.944691308228918, 3.0472675513608003, 2.8857810870269613, 4.55686352612352, 2.794994496288117, 2.8372646277784406, 2.9341125273645243, 2.9334543782053517, 4.824385082349181, 4.571452219927977, 2.9747668163326284, 2.894393876616512, 3.023380301916979, 3.000727447156159, 2.972809529516567, 2.962144163309796, 2.905909261906655, 3.0763076361470314, 3.2255080710136386, 2.8956202342345034, 2.826634367339705, 3.056899093940777, 2.9696467104118045, 3.0161647647057532, 2.9116846755039427, 2.996591115012728, 2.750129484380566, 2.876315622173675, 5.054305120099185, 4.9393700334826125, 2.8452119990298064, 3.0026425249533717, 2.8133392685530745, 3.0197526221258606, 2.7721535518670537, 2.7203183964656303, 2.962582436477175, 2.9024582971224415, 3.0136505666202322, 2.849577607320888, 3.1299149285662713, 4.844014073368962, 3.052995759000381, 2.948124777279832, 2.846611618306464, 4.473856095679582, 2.8950636739214066, 3.0090513313807037, 3.029105716623083, 2.9685892439810706, 4.610278078771599, 2.81265999830531, 2.962816960848723, 2.9751243164678614, 2.847315746914558, 2.9053315475268664, 2.9798046366635647, 2.7435575517752255, 4.483297352829287, 4.693697476174962, 4.476730686245543, 2.885078527906981, 3.1770415485552292, 3.1101729349375593, 2.8017662214966217, 2.8476946231543177, 2.963373115723024, 2.9451591767732426, 4.676144574508071, 2.9877892276938245, 4.538683963059193, 2.754997061818608, 3.088491513938601, 2.8229517609280297, 3.0573828044697615, 2.7954789639732867, 2.7906868115626535, 4.315578424064589, 2.892007552650569, 4.466448464706006, 4.436173895752216, 2.977408663882893, 2.896510753366682, 2.7755466198626286, 3.139846106509705, 2.9486925706110294, 3.01131928118861, 3.1034502412888068, 4.686418002482727, 2.968036013132517, 3.0590266113106104, 2.8052804445097803, 2.8941311225782034, 2.708559555054009, 2.9723134084185276, 3.026693726627666, 4.506059087242703, 4.4957793845297, 3.1429153134290018, 4.437185800441391, 4.262109281780836, 2.9000364001700127, 2.8017837000317383, 2.8318507942750326, 3.079026732086009, 2.583058826300342, 2.9876956530242325, 2.9830171920993918, 3.076354651869152, 4.57770981065308, 4.209069860300178, 2.889666985701954, 2.9670056242897522, 3.0192345578949173, 2.795510736820058, 3.00640363481704, 4.539707602502121, 3.0543470144337785, 2.8460461227649665, 2.8965461601018907, 2.758752084925246, 2.8059266335004223, 2.9284355589947233, 2.9163554822267206, 2.955699498027582, 2.928525564539028, 2.905360074974906, 2.817869815683883, 3.048147931347476, 2.9204948367728027, 2.895384440879908, 2.766056187591696, 2.752656718130114, 2.9596073960374327, 4.5977028838269, 4.690813086502921, 2.8385271563341745, 2.97293197341268, 2.878080701772672, 4.774629020841815, 2.6619107424990527, 2.9294552711600605, 3.0902704529303304, 2.841647956829188, 2.672556039819565, 4.601220096723644, 4.52604591831767, 2.7496505150089283, 2.8903318691910576, 4.554303289804244, 4.383134458244006, 3.1218921533353154, 3.12716090894237, 3.0766673138604896, 2.992744375621381, 3.104423412086555, 4.649009793566452, 3.155394837951631, 2.881350889264103, 2.9923795487666442, 2.955396428188627, 2.9252141535408813, 3.0330171494209868, 2.7797949608667136, 3.0571827813708374, 4.587164853346868, 2.8591130749042013, 2.806716178825869, 2.66486040644328, 2.8007309908167857, 2.8924436019484405, 2.7859457344416803, 2.9938001874034392, 3.0701772972783297, 4.6131722684765215, 4.609331718167743, 2.7788171521883895, 2.800886483343036, 2.994934442559588, 2.77668550673284, 4.718001963320987, 2.9734646319721327, 2.8140662368086184, 2.9273133533235507, 4.453481804044558, 2.6871639604391295, 2.90082250795221, 2.8326223058666704, 2.938850879973146, 2.8905103956976164, 2.8301508159801676, 4.407779589134391, 4.549455059907544, 2.8544024796541643, 2.958674754288913, 4.45677771409448, 4.419787770225888, 2.85219682459742, 2.898350408345726, 3.1078375568952286, 2.9408330761347754, 2.8316543778017627, 2.8461247205289513, 4.454550708915728, 2.8501241062340275, 4.369158622702253, 2.9269953011097614, 2.8727118631939166, 2.876638669991273, 4.317042350769043, 4.487592242393397, 2.960950447489386, 2.866603903950275, 2.9454364623991687, 3.0212995765013257, 2.9134157974327954, 3.106675467363093, 2.906850762576785, 2.5980435034538196, 2.991689101969192, 3.005929657809609, 2.64966148843429, 3.0627912478062687, 2.990559694304033, 3.1067803048506017, 2.872855531424775, 2.901908423774731, 2.9581438452167355, 2.8627020306103392, 2.7751405589506226, 2.7995591524993, 2.9692675267325828, 2.8160890394417697, 2.655231289239477, 2.8159248573788602, 2.976074720853834, 2.7803980156050216, 2.762564477280766, 2.868146486644032, 2.828159974637181, 2.6539944589099624, 4.799490376540906, 2.647066755901058, 2.7981999117509475, 4.757293935414332, 2.9170094021019484, 2.7745107081030573, 2.9567777793854475, 3.0567149430081657, 2.671879949879788, 2.651537551728452, 2.7657585942708867, 4.56642495113902, 2.733656543702552, 2.7937336061427844, 4.58663389678874, 2.936062783328364, 2.9203411550988676, 2.868606135994196, 2.933905635734391, 4.416227302503526, 2.9431393336843574, 2.8374765492822487, 4.632430942558056, 4.261530131995678, 2.7634239285815094, 2.787086445201527, 2.953539830632508, 2.9971988054359344, 4.282321475840435, 2.909184391378898, 2.9039628121120264, 2.8351275322298126, 2.598173101655614, 2.8270523342421243, 3.0275962867927864, 2.757900943133371, 2.770193349785859, 2.7132097684232837, 2.7595130804109087, 2.7443781984850784, 2.630678189378331, 2.8724658999704658, 2.904753321030415, 2.739542862684157, 2.8763996258208913, 2.7591235461498527, 2.756748253031738, 2.8870588899399183, 4.693867912682731, 2.8123435440074465, 2.8405520671129483, 2.93348356363602, 2.9349736361182295, 4.673331519780401, 2.6664032417849692, 2.6668164370550422, 2.9108669474970266, 2.736876191143514, 4.53851874647069, 2.9486352020143274, 2.858104114729616, 2.7718028324823942, 2.6061857199798326, 2.739289003385742, 2.8809888269053774, 2.8596896136686265, 2.8001504280815825, 2.7041190047088444, 2.7096176922547093, 2.8309716046751703, 4.536546410240742, 2.8005010933409946, 2.723691213054254, 4.678800097741824, 2.4901789181959906, 2.9116984526981207, 2.762021172682292, 2.8264278778794343, 2.7086975300946654, 2.678520570174154, 2.724101390551638, 3.018354661964772, 2.5150893626199315, 4.66460096736237, 2.684788169191763, 4.534696267479621, 2.8125080233020827, 2.8064752388291825, 2.930023147522742, 2.704247492170394, 2.7156096214236634, 2.8919068523508855, 2.6944976809781, 4.329143362088469, 2.929754112458172, 4.500782494842623, 2.8023842967586288, 4.473261152666928, 2.66114785095741, 2.8598334676987296, 2.691023032516241, 2.7239328702560943, 4.185298796728621, 2.8788639132063185, 2.6362175851510274, 2.7784204079961654, 2.76202946444268, 2.639548322447113, 4.485924575100306, 2.710548544903056, 2.8350909746338413, 2.7494752623718206, 2.801056985461559, 2.672380784702, 2.8418906671916924, 4.565015076542619, 2.762047329281525, 2.8724496816506697, 2.7405990585591296, 2.979050458750377, 2.605177744274384, 2.8052242890950168, 4.481851454909521, 4.363867471038337, 2.669332724716819, 2.8555129758323785, 2.603194571859209, 2.884114873532578, 2.801242982789631, 4.351550467947385, 2.8558272892669927, 2.873910411438031, 4.38185913200694, 2.87636402482922, 2.773803219567082, 2.6554024935543383, 2.8109758791774513, 2.838554922910161, 2.6702882047484957, 2.6233192898180597, 2.6421309640522925, 2.6885364628898034, 2.8754715690707737, 2.7595946791268466, 2.744638055390079, 2.559053353463984, 2.7962300384823138, 2.8688737294068196, 4.746271168123154, 3.0669657385772515, 2.6660842613320694, 2.7835594457460027, 2.727384784193926, 2.6817729269695403, 2.8029041627289906, 2.746993536595844, 4.450719524278, 2.7411492385489304, 2.738647601429682, 4.499300852515212, 2.6519574647609727, 2.8026729511945865, 2.913303329356864, 2.8853824806276704, 2.774302240651528, 2.82908562918758, 2.633783503355029, 2.622794710060224, 2.8392408021345266, 4.4794387944553, 2.625682973159171, 2.7137502574591092, 2.770009286690023, 2.634114085794735, 2.8023745581891393, 4.7354322560315065, 2.6798645732254918, 2.7047392155731744, 2.6339883568382265, 4.334022055447179, 2.861123924311182, 4.659319504374174, 2.7566225757009315, 2.8099806169349875, 2.586003198207616, 2.5813088062776117, 2.6404959122583116, 2.698149709912795, 2.633253208265955, 2.677514955436258, 4.724083656921532, 2.732939953012253, 4.547545981376282, 2.647653234405803, 2.6036583260366113, 4.2036774012887355, 2.6663735299915463, 4.308879295186181, 4.42736755972518, 2.645612982900366, 2.820665636519694, 2.908118304707537, 2.748364620465076, 2.657610225430991, 2.8178365760703237, 2.7539994072734637, 2.8068932663163157, 2.6745207980116805, 2.771488436948465, 4.423494773030661, 2.846344831200218, 2.619917658567019, 2.572180896416132, 2.8370159968567603, 2.738800790869385, 4.309770862142483, 2.5340801678305747, 4.3915104041765405, 2.641810942366768, 2.681324175342358, 2.875771514466524, 2.7034091893240015, 2.7271449320510612, 2.5234429036440114, 2.6893633745974386, 4.241024874054987, 4.501355332722551, 2.749454821295027, 2.724416746063998, 2.561960909117867, 2.536862378797634, 2.638317972286881, 2.696406286569114, 2.6858503720902194, 2.803296108181159, 4.25612466670478, 2.972595620122559, 2.6387163194640997, 2.7022066783606626, 4.209648817889533, 2.5043113848876297, 2.702519412732885, 2.52812713576318, 2.579411309198063, 2.672065119192363, 2.6029127571357926, 2.709315745593815, 2.619563264628301, 2.639730873941126, 2.4508678940913766, 4.628502476120983, 2.637486387478961, 2.6135449623772313, 2.5225100443131474, 4.659119734760999, 2.8355532687328773, 2.511661590735133, 2.3942639590869335, 2.727025398049732, 2.530573980527647, 4.435594789152278, 2.8013179929097958, 2.5263653876361825, 2.8742240059085753, 2.7739390038065754, 2.599583309956663, 4.484115747110975, 2.5149015226652316, 2.5764289848822113, 2.863176802481381, 2.693156203751283, 2.813509085455497, 2.470664330710148, 2.5876924921058775, 4.451995162886602, 4.2230167072948985, 4.482380171328746, 2.5846393846321285, 2.8039853766671206, 2.645919636400236, 2.810413052683178, 2.8353794657912705, 2.8140019173557946, 2.59664695203661, 2.6742723242792286, 2.57489588951988, 2.6432721004466795, 2.7275173392080134, 2.7508050252056724, 2.879538625665009, 2.745318587698024, 2.5882442170401685, 2.796162320069597, 2.6866743765547687, 2.901590303441818, 2.5942911778662214, 2.757355593269221, 2.700881199762906, 2.4424822668106327, 2.4680930013259252, 4.683280099468466, 2.649684175258816, 4.528014494773829, 2.743958768910492, 2.6010568842465327, 4.476850430441411, 2.690917697672091, 2.7277946489179965, 2.4842887246007717, 2.656440825555968, 2.639953662699554, 2.6811122067366973, 4.512744872179365, 2.599095072269754, 2.6727492391789367, 2.6450853433943524, 2.6401160553337903, 2.6236565814403376, 2.5193756609352285, 2.658714901919516, 2.577805055239511, 4.394438854993441, 2.828816266707318, 2.473855371278218, 2.5517050008159106, 4.247031552232945, 2.7698096688091756, 4.325533211231232, 4.0777008621589, 2.584111959371322, 2.649405713111465, 2.6080413486431295, 4.380292139660163, 2.6355774799025617, 2.5974206315176347, 2.813806120958179, 4.13864800892521, 2.563347811579558, 2.4700106296199924, 2.61022267726764, 4.2425912258479865, 4.392201916867818, 2.877118468231766, 2.6125211844790566, 2.5263303909870913, 4.34562927950526, 2.6573547740920613, 2.560025114034079, 4.2203272215046, 2.50762398539111, 4.396451498844005, 2.6648857711046663, 2.701879206060615, 4.207782117863121, 2.864589597953274, 4.156163322867839, 2.5545476868880534, 4.176666484177493, 2.7218039120252953, 2.7716876311513428, 2.8098652047241077, 2.818114699912545, 2.906888706466847, 2.6323075260076068, 2.8095694063040306, 2.5295179207406697, 2.833470951334993, 4.262757993750274, 4.383737883661248, 2.5858633058463654, 4.509352438875384, 2.877958289834653, 2.6854811333345645, 2.865933947545138, 2.6660289526136998, 4.243301028293549, 2.6668236329520916, 4.220683604282768, 2.7594019359784183, 2.405148574202579, 2.8352793769513664, 2.699149937786973, 4.362205714583397, 2.69536214524157, 2.8072399329567728, 3.865744015593934, 2.693138411730111, 2.6664205289143137, 2.566273210734971, 2.5610029779140304, 2.8355555331621902, 2.766843442286679, 2.68414176915242, 2.6951045870326524, 2.7997989297432957, 2.530870047552187, 2.6066000183917564, 2.7314580968498587, 2.7358222756524015, 2.6595066812136494, 2.5220356499383043, 2.695781911399772, 2.538632164423789, 4.488808363749748, 2.500745652336395, 2.8330637961031337, 2.472991294485038, 2.625856748285702, 2.680921741853924, 4.539492811902792, 2.507833665331789, 2.757715096957383, 4.300603205100038, 2.6052812115622817, 4.42726200010769, 4.258788301037342, 2.6015277157162693, 2.4208206824742082, 2.485491207483567, 2.8160291054598785, 2.606422132488608, 2.6209033723949764, 2.577054964198082, 2.6908935987095854, 4.209972814604164, 4.163166518217138, 2.5013602323105935, 2.635518517910884, 2.586806746016492, 2.9498420085236243, 2.7819289865783916, 2.652545779751758, 2.7562818632642223, 4.572056611905615, 4.165049131177706, 4.181848521433867, 2.9417086582824457, 2.531784692784009, 2.719752686668321, 2.5839824784963112, 2.670825620053674, 2.6691395566263734, 2.703902899583023, 2.7396921808406387, 4.106901134521543, 2.695224016920988, 2.604379417288843, 4.194007036317452, 4.123799806904698, 4.149367691765369, 2.644973606813744, 2.6128069002841032, 2.6091485747642684, 2.544678527130252, 2.766816649936223, 2.6571717801319945, 2.553288511474487, 2.651237235374321, 2.596582055729943, 2.622917806574845, 2.8016851037343344, 2.556736955225111, 2.57951508896727, 2.562782076949203, 2.680680811502811, 2.4494876293060037, 4.51006953444662, 2.530883183698111, 2.3285324447540847, 4.095627653047696, 2.441189743608276, 4.149969695293392, 2.686678253810106, 2.592619002642751, 2.777159204546348, 2.739575174255449, 2.71030878482543, 2.2935222949617398, 2.73655953167581, 2.6707806759136976, 2.521448032169777, 2.5629979443040716, 4.51720792408003, 2.592659457388454, 4.356931112902261, 2.586662411285383, 2.6225791198994557, 4.408847885509136, 2.518632086355867, 2.832531823857117, 2.4486030522208564, 4.213881187029183, 2.5594780524349825, 2.8924486750064085, 2.5603894731251335, 2.4781788738054202, 2.695594232222613, 2.6031567261209068, 2.4431759743577763, 2.5773476536835997, 2.638163000263829, 2.471826745290777, 2.577078334237478, 2.523331184025256, 2.4583825673786412, 2.5943836522963775, 2.4972069277842803, 2.5857445024609995, 2.4862258663485126, 2.768741504253042, 2.5149738557425874, 4.538519838206994, 2.365759134932299, 2.362733142503936, 4.454374479337389, 2.462084512661677, 2.4091329437942477, 4.443041366395768, 2.474676871733544, 4.2653211842609355, 2.6022594228380544, 4.267860393515047, 2.4687171272042154, 2.616330307871354, 2.7574893203727484, 2.615798660436291, 2.446132945658585, 2.5123709734293898, 2.5255875606835376, 2.6941815647950063, 2.5838106442214115, 2.5256353315595446, 2.415546307534239, 2.6472391688157266, 2.7042247482065886, 2.4511420377180912, 2.3275605266348753, 4.220724903391244, 2.4524670909536113, 2.657905787090482, 2.412310751932721, 4.4659636843261445, 2.595001208492698, 2.5914621472609443, 2.6747171825019573, 2.4561913216988596, 2.671327747434855, 2.470754740517798, 4.594613870827131, 2.502859893579816, 4.191799380105741, 2.490522646158934, 3.9839672675528606, 4.203354081385249, 2.5308234981261193, 2.7054012625753505, 2.6239870151416858, 2.702453852353389, 2.748029617715056, 2.7470907139762932, 2.735011459509913, 4.134090524911881, 2.4577778593442177, 2.4568883635907195, 2.572642589299963, 2.469766974692546, 4.362466697546448, 4.250784881175703, 4.130295020822044, 2.353054480050421, 4.2161106796210595, 2.481129416765705, 2.473019282417573, 2.5038180987573297, 2.5246908296626875, 3.829745809058225, 2.588330131998092, 2.5907062198830118, 2.637032914025088, 2.5996190293930854, 2.453944953478346, 2.581155964300893, 4.054430058941697, 2.61678777574031, 2.5831002311009414, 2.5477137404332457, 2.562760667366957, 2.448491574162393, 2.7599598110078754, 4.0821336035487015, 2.435550436933682, 2.389778672540923, 2.2864052504492602, 4.493888492743174, 2.381346362376183, 2.415013668892585, 2.4000678811350773, 4.4618924412503835, 2.4813195169629694, 2.680391036810126, 2.4880880398377205, 2.7088027042897385, 2.4691295407730567, 2.4247528397575464, 4.2046467553144735, 2.502724590885846, 2.4509652156382797, 2.4063490204035474, 2.598412422319137, 2.6355518489203913, 2.3850319091266887, 2.4752578457387595, 2.585041457111014, 2.5740508236312527, 2.5979200075940967, 2.4035631745763104, 4.296742623825015, 2.458721100005098, 2.4728137438639415, 2.446579058444484, 4.37677145569692, 2.3914782610497896, 4.437323441758881, 2.598371492240598, 2.4182981942674795, 2.588082121541867, 2.620196659174097, 2.3361062512511297, 2.4219256090237473, 2.780311886623701, 2.2622774970403254, 2.444116244171647, 2.510774944222493, 2.579266551396917, 2.3364918962334995, 2.4196233373507856, 2.772911733023211, 2.461682773778971, 2.338999221705873, 2.604859702187402, 4.395404562645126, 4.083010451766405, 2.329231049982632, 2.6311236986639877, 2.4611381662884084, 2.426736095439372, 2.450253354027647, 2.3702219984556883, 2.6640178945435875, 2.352855991557248, 2.582862997550498, 4.488837167774477, 2.5202453793141113, 2.3033605902079457, 4.175538755823897, 4.321733620555643, 4.24909205398762, 2.4810178043642077, 2.456896512552795, 2.558474287191786, 4.158257857484011, 2.2363107003154554, 2.622374392877485, 2.756359524143059, 2.1721668275647192, 2.5022995918491344, 2.586015312172953, 2.3808145082778394, 4.063921178642072, 2.5290325084903853, 2.42785529126302, 2.37167666695821, 2.5123223519944524, 2.5771463685340232, 4.287953529290055, 2.298256382955231, 2.6961521646544746, 2.472071713832922, 4.012308333009322, 4.1120503016559455, 2.57488622930959, 4.032193470429629, 2.52425071644209, 2.5753270293907686, 2.4759173962808014, 2.4503033546320663, 2.4115355443760453, 2.449923972295904, 2.476137866188079, 2.690590168716177, 2.3899520908196297, 2.4105558173491937, 2.3685456594435306, 2.491105335524015, 2.4783355752569713, 2.530855668133733, 2.7440657476813253, 2.372349963245164, 2.6399221285527315, 2.4543445951863134, 4.18029932898994, 4.109380722821424, 2.6029289377559173, 2.536498633803743, 2.559106800380169, 2.5459658991525935, 4.154809258810055, 2.5510245678956025, 2.488789432632077, 2.381889209618085, 2.4536696690564943, 2.5430054738837176, 2.5649174636807937, 2.536417680422019, 2.473337933793724, 4.382490774802387, 2.5021313194930093, 2.494366066946543, 2.3714167992891806, 2.353207809056819, 2.3972120208066126, 2.479978173635522, 4.187741877777236, 2.275717899749534, 2.4978285926855586, 2.2754425956638316, 2.653620083899134, 2.490254308410959, 2.3127441640028, 2.2975290972047686, 2.5847786816043987, 2.5425051690484044, 2.41301516178254, 2.525932621615057, 2.4290209172024735, 2.2465602174382444, 2.4942305468996264, 2.6061356609717645, 2.3983287827765256, 2.664522282557263, 2.5618057170816297, 4.348832400970151, 2.38517694382179, 2.418271600812938, 2.3202856275083135, 2.62494611498138, 2.397230563626428, 2.4057608376759703, 2.5572249515801864, 2.4036637613932315, 2.545277080823841, 4.231309849953017, 2.3330939123314827, 2.3846769562730206, 4.463879197585783, 2.37055664114678, 2.125854172794771, 2.3481398136579297, 2.341893744990323, 2.5040720823313793, 2.3536460541925113, 2.375842989387226, 2.473256232266334, 2.3880201450361853, 2.4832977882784526, 2.416003809438139, 2.323821131198274, 2.353512118433335, 2.354112960606838, 2.294652117344276, 2.3339709129995936, 2.64008467657401, 2.303649890129936, 4.575968313095086, 2.5301706341085883, 2.405431308085099, 4.28276403852914, 2.180490775999819, 2.5326008212578706, 4.075280983834157, 2.3712437056657887, 2.6549848027527334, 4.031642358674079, 2.4498296099930745, 2.238344829379825, 2.2985609774816336, 2.317080942856616, 4.086875110736339, 2.5002004975213272, 2.520861199944667, 2.381323585243642, 4.220916226450144, 4.145763205155274, 2.399402415206094, 2.571211894061894, 2.3069788010300307, 2.3478631340611127, 2.3762158681901657, 2.503610136515632, 2.4172376769940973, 3.9951200298731857, 4.112421265291294, 4.494734230786562, 4.270971215753175, 2.3630024820932443, 3.911342809986978, 2.8389260172526694, 2.406829894085404, 2.559429514296831, 3.959437282535902, 2.3851541567193655, 2.374652167257663, 2.430030996035255, 2.505774231756867, 4.028280030705712, 2.5239733811684752, 2.444394619633424, 3.8752506616814384, 2.5733927866788604, 2.6271305512378524, 3.846039750405398, 2.414019444435252, 3.881845237184316, 2.3674874873156, 2.422558124099165, 2.490116566632908, 2.4308399126014115, 2.3372648056286858, 2.4376885865648474, 2.4082666613110275, 2.584131540204451, 2.495678368218262, 2.493379068556171, 3.885798649190907, 2.5267028082467307, 2.4169249886781774, 2.430396778543423, 2.445746289151166, 2.490938469982116, 2.499149076575168, 2.336853305518011, 2.37191313075189, 4.256367776894685, 4.516592815659623, 2.4585233412529757, 2.371468148318818, 2.588614463581229, 2.373676905700946, 2.3610693574762407, 2.3332696818933365, 3.924387214973474, 2.6159378252461294, 2.3483859047104243, 2.2361917640805613, 2.3868954130988738, 2.3990917834673415, 2.3267032628037683, 2.1601237737001426, 4.073782955756931, 2.4200419722071786, 2.442306132020807, 2.3501341324394125, 2.3461840921479045, 2.374862659635369, 2.464837782600929, 2.6120144076089087, 2.2703984120536655, 4.439139681323882, 4.368588019806053, 4.148827740080288, 2.2642310819716034, 2.5972781953418287, 2.311958884155615, 2.3163278503778195, 4.03921989361807, 3.5707892108484907, 4.133826682119237, 2.544098837442132, 3.946613429432706, 2.4823584635389326, 2.377846748191154, 2.72325584639064, 2.425364024395113, 2.5363009699938917, 2.45001623800929, 2.3875664352817307, 2.4555914357800006, 3.641869749291652, 2.5310456961279315, 2.410387094639461, 2.29298491041198, 2.386979573527576, 3.8690590506669174, 3.8655717977229624, 2.324883636876805, 2.515038584116529, 2.379000238743909, 2.3427283745191643, 2.2239232320117086, 2.449098393853222, 2.424502605192354, 2.270642641784236, 2.1944963038002174, 2.423852932968819, 2.405738387155683, 2.2095759508005384, 4.276635233526737, 2.404009616945209, 2.45363018870944, 2.3744786839691265, 2.416622668584574, 2.551781608635422, 2.3465126554759532, 2.5007422575880165, 2.3040493532225037, 4.3381342485473295, 2.197197282065948, 2.2927250083358515, 2.349845168764423, 2.233815814727737, 2.2891452061725133, 2.17933995774035, 4.371111581287574, 2.2991610789018573, 2.203662788194694, 2.2654425741855984, 2.341678067462833, 2.2222977669056876, 2.086607734068316, 2.4921649267814656, 2.3719888877780457, 4.168608249252008, 2.452647149935035, 3.9377297608919863, 3.9559645677168365, 2.299524359048134, 2.436994896529167, 2.4471463688295407, 2.3226612501343396, 2.3312600941892234, 2.305727986093745, 2.2670033052835588, 2.363583445861822, 2.3152288967544288, 2.3282100535281747, 4.1926555205501765, 3.841921438798496, 2.2961732317453056, 4.223564179335595, 2.3807254327280867, 2.3685950074165016, 2.358222401401518, 2.5179431313216836, 2.388749653480336, 2.4418116287385274, 2.564879862481262, 4.087816251946538, 3.9915020071017513, 2.3446774583287033, 2.2798589137913305, 2.309192029844191, 3.8708504262574754, 2.3869652031136996, 2.128259290446187, 2.1715574614012363, 2.163869466787825, 2.273980102417459, 3.760469114097456, 2.4344644556874813, 2.571482138749092, 2.2858751972951232, 3.9625210292226085, 2.280603261480397, 2.4942952321201877, 4.107429460065449, 2.3344863735205137, 2.29407252019812, 3.9911766204048185, 2.511270834323, 2.3769010587261534, 2.4003239814174417, 2.3704916402809464, 2.164177147003538, 2.3733948910764107, 2.4187786625511705, 2.367473838832035, 2.350468697178896, 2.3267412582497458, 2.470571327013036, 2.5981147098452007, 2.231576758393295, 2.3680573150484787, 2.3263671976425844, 2.3717443868735795, 2.315453462972376, 4.329422898301319, 4.314653105753705, 2.336785401979523, 3.8720940350767212, 2.4065544687155, 4.090024159789086, 2.5552714435971358, 4.221516632691376, 2.4426924460526327, 2.350519624917801, 2.3462616206297864, 2.518328123207457, 2.419311948085439, 2.2322619618790487, 2.4890082495163197, 2.2704643292144313, 2.34743085388944, 2.5836702240774945, 2.4939360668492094, 4.004710370488229, 2.377221893224113, 3.840565082769668, 2.5606233745385496, 4.2286851234778675, 2.3915574767074927, 2.409499013687405, 4.105633881932157, 2.318770645754013, 2.4021574653741466, 2.3284095034708168, 3.9293349027086837, 2.2135719029884786, 2.4417124154777503, 2.322809733648041, 3.7791808825686464, 2.4674539530569066, 2.3226726826393875, 2.4542326357736357, 3.7792339920247877, 2.3179476613700865, 2.2375734233719875, 2.35902780760327, 2.3245729248263287, 2.6407399392195723, 3.9611439337333043, 2.4742842753752257, 2.415240879379695, 3.973105756656348, 2.2691551107865497, 2.366450636927503, 2.395015836052706, 2.203565694446377, 2.161224660971793, 2.2155728577607543, 2.2557825111399215, 2.3221576588547386, 4.053455417864173, 2.353514851971573, 2.3508297651003724, 2.2251688992213103, 2.215037733972687, 2.0836881881722236, 3.9016364281584184, 2.1951154065166687, 2.264866062944775, 2.314929335899265, 2.3412091506594366, 2.317003982133362, 2.2474189663790676, 2.1885003906985125, 2.309423430775434, 2.3612671556542053, 2.412988976069859, 2.131374840176394, 2.016446580850995, 4.055215740234075, 3.8447277988618374, 3.851599083776059, 3.990697235134081, 2.362978795545738, 2.496547105928214, 2.427338749094663, 2.1653982184019434, 2.150832794547191, 2.3529102039098, 4.164483535244893, 2.3655270776538, 2.3908848655816914, 2.1936975404006946, 2.5374200776390543, 2.1949915314395914, 2.339120835427358, 2.2083660852378473, 2.4315324459305923, 2.2986864979083106, 2.285623050993592, 2.1653192171815094, 2.2531289227395774, 2.3218313160267745, 2.1180539688067084, 3.9735316243587118, 2.0715710588606453, 2.236870256930781, 2.1825202211955097, 2.222569870978727, 2.4065539044200204, 2.1748312692206295, 2.3012977110906374, 2.172405423558488, 2.3507787610648383, 2.439539888288529, 3.8623750103673626, 2.25367418649368, 2.2994865800936095, 2.178560676044036, 2.4565279588378512, 4.046271619277123, 4.1329453979708815, 2.4236471281632475, 2.124938545817202, 2.1991530274923687, 3.9543778079570346, 2.173116703837443, 2.3840572489784693, 2.198877574373928, 2.462943406027043, 2.1172034113431475, 4.181679663093549, 2.277472898072085, 2.2311289484772696, 3.9112204100734504, 2.1647184924322103, 3.9215968319132357, 2.5227661860841963, 3.784540012235773, 2.3810781290595417, 2.4040224755457205, 2.5961439775179143, 2.3453758768639412, 2.320831539163785, 2.294038255108834, 2.3121212375997247, 2.3071223422450085, 3.8582238364453407, 2.2016614380493897, 2.369309771919015, 4.059042500436004, 2.1188230998627344, 2.334223971956515, 2.034025619428133, 2.3694130454760707, 3.796792043622944, 3.7136503680585284, 2.2415847506646704, 2.5032165303830887, 2.224629610767588, 2.1129449869234427, 2.426165146193481, 2.579752392725772, 2.16989005702814, 2.240143228025037, 2.043683210848662, 2.1795015856756854, 2.1054396796132986, 2.204330342845811, 2.2618168369159575, 2.3774171576342655, 2.0490898116679856, 2.251778171639463, 2.2671211230128927, 4.126365008377533, 2.3084610870284172, 2.268482315485775, 2.148908505763513, 2.212742615707578, 2.3520285778912347, 2.2432082359002243, 2.34121202779851, 1.982364611619329, 2.418288497926293, 4.031313455358128, 3.8556778762933503, 2.1203594823345178, 2.2076413687023266, 2.2547300358065723, 2.4033170897185197, 4.139677594952497, 2.1858940262035857, 2.1356192798044065, 4.011388086449744, 2.287249439030132, 2.0909815639653444, 3.916858176439631, 3.815610041170937, 2.2680595733584763, 2.3489219758342554, 3.795237351222948, 2.414314710233661, 2.30268388233633, 3.778473262711457, 2.4304870976336823, 2.3647629003089263, 2.280095395784312, 2.126371813916222, 2.2352524679777486, 2.429080442038696, 2.265070981287789, 4.171310832294134, 2.3316870822428584, 3.761698157450799, 2.199469258157128, 3.987008276825431, 4.101908765450308, 3.963118023464747, 2.382295689537309, 2.4227527175121852, 2.2333704117808786, 2.207668138024036, 4.06510684848914, 2.5166718215759607, 2.256543761558707, 2.1288750513702097, 2.3684115602292968, 3.5217559531331064, 2.373954906065053, 2.1942985585796326, 3.924214163628049, 2.237941197205586, 2.3060357027745413, 3.785731178734722, 2.383445317619038, 2.2939595548912535, 2.2705007046066896, 2.1095536261686436, 2.325338651738057, 4.0798028346264, 2.2385818733960354, 2.551878804580753, 2.180704871566049, 2.223433200631616, 2.228548574914535, 2.0375340411314817, 2.0832002656021347, 3.9766997906251573, 2.326702476900534, 2.4229106023208558, 2.385884942659137, 2.128879028978909, 3.89860203358906, 2.4162695709856745, 3.9287600195485783, 2.2918107772670817, 2.198843682180239, 2.2082035957699846, 4.061578724437094, 2.043048817681058, 2.1817188521932986, 2.225462447959996, 1.9849518285948142, 2.2124815548055006, 2.143567882104659, 2.04316529289526, 3.7456349747763023, 3.8543780985112646, 2.4110552385587467, 3.78258059254473, 4.144684191146514, 2.209150138698533, 2.4629636547666225, 2.331447093980387, 2.4168321056253355, 2.2999268960120207, 2.3175226440213117, 2.333198954461245, 2.375959651635902, 2.254085635778871, 2.150265485878594, 2.167219133028737, 2.1987358034228, 2.229185424666118, 2.313963694289955, 3.9592059507046216, 2.2465609136915075, 2.1836843602560214, 2.124446566165117, 2.3432956140163053, 4.440388105655789, 2.5378433274810703, 2.3030939358761264, 4.048604549990072, 3.9701812199797564, 3.808217830063103, 2.174298554320654, 2.1011349805975006, 2.1245725225028202, 2.100369584497026, 2.214949779771888, 3.8097495100139263, 2.0258375653187812, 2.2220333117083615, 2.196099604749123, 2.164993526477456, 2.524931571214361, 3.7562453752973273, 3.963297636710889, 2.2421258316271837, 3.7822082129350654, 2.1947881315865785, 2.117808224278089, 2.2010607663993036, 3.658801335874426, 2.22713464624614, 2.326721372874762, 2.380328767814084, 2.320729207184355, 4.0012419206973835, 3.8825053846255617, 2.2091482257059254, 2.071733813998127, 3.8875580943372263, 2.3418375634044146, 2.2412840061186214, 3.785459056807847, 2.2272976901790966, 2.30061055593368]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(training_losses, validation_losses, save_dir):\n",
    "    print(validation_losses)\n",
    "    print(training_losses)\n",
    "    \"\"\"Plot training and validation losses.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(training_losses, label='Training Loss', color='blue')\n",
    "    if validation_losses:\n",
    "        plt.plot(validation_losses, label='Validation Loss', color='orange')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(save_dir, 'loss_curve.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot loss curves\n",
    "plot_losses(training_losses, validation_losses, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: hi . doing investigation\n",
      "Bot: fine . .\n",
      "Bot: i . . . .\n",
      "Error: Encountered unknown word.\n",
      "Bot: i have to .\n",
      "Bot: i have to .\n",
      "Bot: you re . . .\n",
      "Bot: i ll be like you .\n",
      "Bot: because i have to . .\n",
      "Bot: why ? deer\n",
      "Error: Encountered unknown word.\n",
      "Bot: he s right ! ! !\n",
      "Error: Encountered unknown word.\n",
      "Bot: what ?\n",
      "Bot: what ?\n",
      "Bot: what ?\n",
      "Bot: what ?\n",
      "Bot: what ?\n",
      "Bot: what ?\n",
      "Bot: i have to . .\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to ``eval`` mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
